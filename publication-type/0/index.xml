<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>0 | Mingkun&#39;s homepage</title>
    <link>https://mingkunyang.github.io/publication-type/0/</link>
      <atom:link href="https://mingkunyang.github.io/publication-type/0/index.xml" rel="self" type="application/rss+xml" />
    <description>0</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 10 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://mingkunyang.github.io/images/icon_hu786fa58ccadddb09b05070c28e40422f_10189_512x512_fill_lanczos_center_2.png</url>
      <title>0</title>
      <link>https://mingkunyang.github.io/publication-type/0/</link>
    </image>
    
    <item>
      <title>MetricNet: A Loop Closure Detection Method for Appearance Variation using Adaptive Weighted Similarity Matrix</title>
      <link>https://mingkunyang.github.io/preprint/metricnet/</link>
      <pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://mingkunyang.github.io/preprint/metricnet/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Loop Closure Detection (LCD), also known as visual place recognition, provides extra information that &lt;strong&gt;robot has visited this area before&lt;/strong&gt;. This constrain assists the positioning system to &lt;strong&gt;reduce the pose estimation error&lt;/strong&gt; by following optimization algorithm.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The proposed MetricNet contains two blocks: 1) feature extraction leveraging &lt;strong&gt;deep learning&lt;/strong&gt;; 2) similarity measurement with &lt;strong&gt;adaptive weight&lt;/strong&gt;. As an &lt;strong&gt;end-to-end&lt;/strong&gt; loop closure detector, MetricNet is trained by minimizing the binary cross-entropy (BCE) consisting of label and calculated similarity score.
The contributions can be summarized into:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Effective feature selection and adaptive similarity matrix&lt;/strong&gt;: The learning-based feature extraction exploits spatial information. Inspired by inverse documentary frequency (IDF), a channel weight mechanism is devised to distil features that are effective for image matching. In terms of similarity calculation, it considers the link between diagonal and off-diagonal elements of similarity matrix.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accurate and robust detection performance&lt;/strong&gt;: Extensive experiments reveal that MetricNet outperforms both learning-based and conventional approaches, especially under the condition with distinct apperrance changes.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual Odometry</title>
      <link>https://mingkunyang.github.io/preprint/deepavo/</link>
      <pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://mingkunyang.github.io/preprint/deepavo/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Visual Odometry (VO), recovers the ego-motion from image sequences by exploiting the consistency between consecutive frames, which has been widely applied to various applications, ranging
from autonomous driving and space exploration to virtual and augmented reality. Although many state-of-the-art learning-based methods have yielded competitive results against classic algorithms, they consider the visual cues in the whole image equally.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The proposed DeepAVO distinguishes and selects extracted features from two aspects: 1) there are four branches extract geometric information from &lt;strong&gt;corresponding quadrant of optical flow&lt;/strong&gt;; 2) each branch in the DeepAVO contains two CBAM blocks enabling the model to concentrate on &lt;strong&gt;pixels in distinct motion&lt;/strong&gt;. The contributions can be summarized into:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Novel visual perception guiding ego-motion estimation&lt;/strong&gt;: The DeepAVO consider the features in four quadrants of optical flow dividually, and fuse the distilling module into each encoder branch. It makes the learning-based model pays more attention to the visual cues that are effective for ego-motion estimation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accurate and robust VO&lt;/strong&gt;: Our medel outperforms many learning-based and traditional monocular VO methods, and even gives &lt;strong&gt;competitive results against the classic stereo VISO2-S algorithm&lt;/strong&gt;. In addation, the DeepAVO produces &lt;strong&gt;promising tracking results on the cross-dataset validation&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Symmetrical-Net: Adaptive Zero Velocity Detection for ZUPT-Aided Pedestrian Navigation System</title>
      <link>https://mingkunyang.github.io/preprint/symmetrical-net/</link>
      <pubDate>Fri, 23 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://mingkunyang.github.io/preprint/symmetrical-net/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Zero velocity detection selects the phase that &lt;strong&gt;foot is anchored to the ground&lt;/strong&gt; from the whole gait of pedestrian. As the vital part of zero velocity update (ZUPT), it &lt;strong&gt;triggers the error-state Kalman Filter (ESKF)&lt;/strong&gt; to modify the position estimated by pedestrian dead reckoning (PDR).&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The characteristics of proposed Symmetrical-Net, as an adaptive zero velocity detector, contain: 1) leveraging &lt;strong&gt;RCNNs&lt;/strong&gt; extracts the features from waveform image of inertial data, and derives connections among consecutive sampling points; 2) Two identical RCNNs branches are &lt;strong&gt;assembled in a symmetrical shape&lt;/strong&gt;, analysing the raw IMU readings berfore and after the undetermined time instant. The contributions can be summarized into:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Robust and adaptive zero velocity detection&lt;/strong&gt;: Compared with the fixed threshold detector, the proposed Symmetrical-Net can &lt;strong&gt;adaptively calculate the probability&lt;/strong&gt; of being stationary, conserving the manual labour to fine tune the threshold. It also outperforms other competing adaptive detectors, which is attributed to &lt;strong&gt;considering contextual information&lt;/strong&gt; and &lt;strong&gt;extended observation scope&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accurate and robust pedestrian tracking&lt;/strong&gt;: Performance of Symmetrical-Net makes a profound &lt;strong&gt;impact on the trajectory reconstruction of the ZUPT-aided INS system&lt;/strong&gt;. Extensive experiments reveals that the navigation system, assisted with proposed detector, maintains high-accuracy positioning &lt;strong&gt;under various individuals in three motion&lt;/strong&gt; (walking, fast walking, and running).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Visual Inertial Map Matching for Indoor Positioning using Architectural Constraints</title>
      <link>https://mingkunyang.github.io/preprint/map/</link>
      <pubDate>Sun, 07 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://mingkunyang.github.io/preprint/map/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For the self-contained navigation systems, the accurate &lt;strong&gt;initial position&lt;/strong&gt;, as the prior knowledge, is crucial for real-world task. Whereas, most existing systems rely on the external signals to achieve initialization, which is contrary to the original intention of positioning whithout pre-installed infrastructure.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The proposed algorithm requires 2D plane map, foot-mounted inertial data, hand-held shot images, realizing a real self-contained navigation system. From the topological points in the maps, the trajectory of pedestrian can be selected via three steps: 1) cursory selection by &lt;strong&gt;ZUPT-aided INS&lt;/strong&gt; generates several candidate paths; 2) &lt;strong&gt;door detection and matching&lt;/strong&gt; excludes some trails further; 3) the final result is decided by a &lt;strong&gt;Siamese Network&lt;/strong&gt; matching the spatial structure.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LightVO: Lightweight Inertial-Assisted Monocular Visual Odometry with Dense Neural Networks</title>
      <link>https://mingkunyang.github.io/publication/globecom/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://mingkunyang.github.io/publication/globecom/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Visual Odometry (VO), conducts the ego-motion estimation using on-board camera. In this paper, a &lt;strong&gt;learning-based monocular VO&lt;/strong&gt; is proposed. What&amp;rsquo;s more, with the IMU correction through &lt;strong&gt;loose-coupled mechanism&lt;/strong&gt;, a visual-inertial odometry improves the accuracy of pose estimation further.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The input of the proposed VO is optical flow extracted by TVNet, and the structure of the learning-based VO is inspired by the DenseNet that is lightweight and effective neural network mainly applied to CV feilds. Utilizing the &lt;strong&gt;KF&lt;/strong&gt;, the inertial positioning is merged into the navigation scheme. The contributions can be summarized into:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lightweight and accurate visual odometry&lt;/strong&gt;: Both parameters amount and average execution time of the proposed VO is lighter than competing approach. In addation, the proposed VO outperforms other VO in translation estimation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Loose-coupled visual-inertial odometry&lt;/strong&gt;: The well devisd data fusion algorithm considers the accumulated error of INS. Therefore, the &lt;strong&gt;penalty for translation estimation&lt;/strong&gt; by INS is set, which avoids excessive correction. Compared with the proposed visual odometry, the visual-inertial decreases the translation and rotation error by up to &lt;strong&gt;70%&lt;/strong&gt; and &lt;strong&gt;73%&lt;/strong&gt;, respectively.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Efficient Human Activity Recognition Solving the Confusing Activities via Deep Ensemble Learning</title>
      <link>https://mingkunyang.github.io/publication/access/</link>
      <pubDate>Sat, 01 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://mingkunyang.github.io/publication/access/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Human activity recognition (HAR), &lt;strong&gt;identifying the actions of individual&lt;/strong&gt; based on a set of observation, is widely applied to pedestrian indoor tracking, healthcare, Intelligent city, etc. Vision-based recognition algorithms suffer from factors such as lighting condition, clothing color, and image background. Therefore, this paper, leveraging the &lt;strong&gt;build-in sensor of the smartphone&lt;/strong&gt;, proposes a sensor-based HAR model.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The proposed model utilizes CNN to perform the HAR task. A CNN-7 block is used to identify seven activities. In terms of confusing human activities, a binary-classifier CNN-2, in the &lt;strong&gt;ensemble learning&lt;/strong&gt; way, helps the model to determine the actual human activity through &lt;strong&gt;weighted voting&lt;/strong&gt;. The contributions can be summarized into:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Abundant and extensive date collection&lt;/strong&gt;: The data in this paper contains three motion sensors readings collected by 100 participants. There are &lt;strong&gt;four different placements&lt;/strong&gt; of the smartphone and &lt;strong&gt;seven typical daily human activities&lt;/strong&gt;. The data set not only guarantees the effective training on neural network, but makes the recognition task challenging.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accurate and robust recognition&lt;/strong&gt;: &lt;strong&gt;Avoiding handcrafted features engineering&lt;/strong&gt;, the average accuracy of proposed ensemble model can achieve up to &lt;strong&gt;96.11%&lt;/strong&gt;. The devised voting mechanism &lt;strong&gt;improves the classification accuracy among two confusing activities&lt;/strong&gt; (walking and going upstairs) further.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The Research of Stance-Phase Detection to Improve ZUPT-Aided Pedestrian Navigation System</title>
      <link>https://mingkunyang.github.io/publication/iscas/</link>
      <pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate>
      <guid>https://mingkunyang.github.io/publication/iscas/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In the &lt;strong&gt;GPS-denied&lt;/strong&gt; environment, inertial navigation system is the justifiable and general solution because it &lt;strong&gt;do not require installation of infrastructure beforehand&lt;/strong&gt;. Nevertheless, the inertial navigation system (INS) calculates the relative displacement of an object that will result in &lt;strong&gt;accumulated errors&lt;/strong&gt; due to the sensors’ drift. Under the assumption that the velocity is zero when foot is on the ground, ZUPT-aided INS modify the positioning error leveraging this pseudo-measurement.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;This paper improve the ZUPT-aided INS in two aspects: 1) By considering the zero velocity detecion as a hypothesis-testing problem, we propose a new test statistic; 2) To avoid excessive modification of heading angle (yaw), an asymptotic ZUPT is devised.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Accurate zero velocity detection&lt;/strong&gt;: This paper takes the &lt;strong&gt;difference value&lt;/strong&gt; of gyroscope as the test statistic, which makes the wave more sharper. The proposed method enable zero velocity detecion to maintain its accuracy &lt;strong&gt;in high-dynamic motion&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stable attitude modification&lt;/strong&gt;: The heading angle, with zero velocity update, is prone to deviation, because the excessive yaw error estimation by KF at the initial update. Therefore, the &lt;strong&gt;attenuation factor&lt;/strong&gt; is applied to the pseudo-measurement, which alleviates the yaw fluctuation after update.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
