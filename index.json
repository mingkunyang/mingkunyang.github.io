[{"authors":null,"categories":null,"content":"I am a PhD candidate co-supervised by Dr. Qing Wang, Dr. Jie Yang, and Prof. Koen Langendoen at the Embedded and Networked Systems (ENS) group, faculty of EEMCS, Delft University of Technology.\nBefore that, I obtained my bachelor and master degree in the School of Information and Communication at University of Electronic Science and Technology of China (UESTC), where I was supervised by Dr. Zhuoling Xiao.\nMy research was focused on Pedestrian Dead Reckoning (PDR), visual-inertial odometry (VIO). I am now intrested in leveraging the power of human to undertand and interpret the artificial intelligence models. All my works aim to make very positive contributions to improving the performance and robustness of machine learning systems.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://mingkunyang.github.io/author/mingkun-yang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/mingkun-yang/","section":"authors","summary":"I am a PhD candidate co-supervised by Dr. Qing Wang, Dr. Jie Yang, and Prof. Koen Langendoen at the Embedded and Networked Systems (ENS) group, faculty of EEMCS, Delft University of Technology.","tags":null,"title":"Mingkun Yang","type":"authors"},{"authors":null,"categories":null,"content":"","date":1635120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635120000,"objectID":"6bc20f55358e40b3b91245fa92c59e74","permalink":"https://mingkunyang.github.io/awards/map/","publishdate":"2021-10-25T00:00:00Z","relpermalink":"/awards/map/","section":"awards","summary":"Outstanding graduates of Sichuan province (2021) Outstanding graduates of UESTC (2021) First-class academic scholarship * 2 (2019-2020)  Outstanding Student Awards of UESTC * 2 (2019-2020)  The tide of Sichuan scholarship (2020)  Gold award in China Graduate Electronics Design Contest (Top 1\\%) (2020)  Gold award in China College Students Internet + Innovation and Entrepreneurship Competition (Sichuan district competition) (2020)  People scholarship of UESTC * 4 (2015-2018) ","tags":null,"title":"","type":"awards"},{"authors":null,"categories":null,"content":"","date":1623024000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623024000,"objectID":"b4f7176e0d5b024fab6809760af5c2b1","permalink":"https://mingkunyang.github.io/publication/real-time-pdr/","publishdate":"2021-06-07T00:00:00Z","relpermalink":"/publication/real-time-pdr/","section":"publication","summary":"*SENSORS* Ran Wei, Hongda XU, **Mingkun Yang**, Xinguo Yu, Zhuoling Xiao* , Bo Yan","tags":["PDR","Intelligent Recognition","FPGA"],"title":"Real-Time Pedestrian Tracking Terminal Based on Adaptive Zero Velocity Update","type":"publication"},{"authors":null,"categories":null,"content":"Background For the self-contained navigation systems, the accurate initial position, as the prior knowledge, is crucial for real-world task. Whereas, most existing systems rely on the external signals to achieve initialization, which is contrary to the original intention of positioning whithout pre-installed infrastructure.\nContribution The proposed algorithm requires 2D plane map, foot-mounted inertial data, hand-held shot images, realizing a real self-contained navigation system. From the topological points in the maps, the trajectory of pedestrian can be selected via three steps: 1) cursory selection by ZUPT-aided INS generates several candidate paths; 2) door detection and matching excludes some trails further; 3) the final result is decided by a Siamese Network matching the spatial structure.\n","date":1623024000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623024000,"objectID":"ba5160d4e76ca9e1e076b603f5af6de6","permalink":"https://mingkunyang.github.io/preprint/map/","publishdate":"2021-06-07T00:00:00Z","relpermalink":"/preprint/map/","section":"preprint","summary":"Kunxin Li, **Mingkun Yang**, Ran zhu, Heng Lian, Nan Wu, Zhuoling Xiao*, Bo Yan, Liang Zhou","tags":["Map Matching","PDR"],"title":"Visual Inertial Map Matching for Indoor Positioning using Architectural Constraints","type":"preprint"},{"authors":null,"categories":null,"content":"Background Loop Closure Detection (LCD), also known as visual place recognition, provides extra information that robot has visited this area before. This constrain assists the positioning system to reduce the pose estimation error by following optimization algorithm.\nContribution The proposed MetricNet contains two blocks: 1) feature extraction leveraging deep learning; 2) similarity measurement with adaptive weight. As an end-to-end loop closure detector, MetricNet is trained by minimizing the binary cross-entropy (BCE) consisting of label and calculated similarity score. The contributions can be summarized into:\n Effective feature selection and adaptive similarity matrix: The learning-based feature extraction exploits spatial information. Inspired by inverse documentary frequency (IDF), a channel weight mechanism is devised to distil features that are effective for image matching. In terms of similarity calculation, it considers the link between diagonal and off-diagonal elements of similarity matrix. Accurate and robust detection performance: Extensive experiments reveal that MetricNet outperforms both learning-based and conventional approaches, especially under the condition with distinct apperrance changes.  ","date":1621468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621468800,"objectID":"67a07d93e8dc72cc703ff8fe172bafb7","permalink":"https://mingkunyang.github.io/preprint/metricnet/","publishdate":"2021-05-20T00:00:00Z","relpermalink":"/preprint/metricnet/","section":"preprint","summary":"\nYing Li, Ran Zhu, **Mingkun Yang**, Zhuoling Xiao*, Yuhan Zhang, Bo Yan,","tags":["Image Matching","Intelligent Recognition"],"title":"MetricNet: A Loop Closure Detection Method for Appearance Variation using Adaptive Weighted Similarity Matrix","type":"preprint"},{"authors":null,"categories":null,"content":"Background Visual Odometry (VO), recovers the ego-motion from image sequences by exploiting the consistency between consecutive frames, which has been widely applied to various applications, ranging from autonomous driving and space exploration to virtual and augmented reality. Although many state-of-the-art learning-based methods have yielded competitive results against classic algorithms, they consider the visual cues in the whole image equally.\nContribution The proposed DeepAVO distinguishes and selects extracted features from two aspects: 1) there are four branches extract geometric information from corresponding quadrant of optical flow; 2) each branch in the DeepAVO contains two CBAM blocks enabling the model to concentrate on pixels in distinct motion. The contributions can be summarized into:\n Novel visual perception guiding ego-motion estimation: The DeepAVO considers the features in four quadrants of optical flow dividually, and fuse the distilling module into each encoder branch. It makes the learning-based model pays more attention to the visual cues that are effective for ego-motion estimation. Accurate and robust VO: Our medel outperforms many learning-based and traditional monocular VO methods, and even gives competitive results against the classic stereo VISO2-S algorithm. In addation, the DeepAVO produces promising tracking results on the cross-dataset validation.  ","date":1604534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604534400,"objectID":"7f2c0b8ddeaa87e8c983e6d64041c171","permalink":"https://mingkunyang.github.io/publication/deepavo/","publishdate":"2020-11-05T00:00:00Z","relpermalink":"/publication/deepavo/","section":"publication","summary":"*NEUROCOMPUTING* Ran Zhu, **Mingkun Yang**, Wang Liu, Rujun Song, Zhuoling Xiao*, Bo Yan","tags":["Visual Odometry","Attention Mechanism"],"title":"DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual Odometry","type":"publication"},{"authors":null,"categories":null,"content":"Background Zero velocity detection selects the phase that foot is anchored to the ground from the whole gait of pedestrian. As the vital part of zero velocity update (ZUPT), it triggers the error-state Kalman Filter (ESKF) to modify the position estimated by pedestrian dead reckoning (PDR).\nContribution The characteristics of proposed Symmetrical-Net, as an adaptive zero velocity detector, contain: 1) leveraging RCNNs extracts the features from waveform image of inertial data, and derives connections among consecutive sampling points; 2) Two identical RCNNs branches are assembled in a symmetrical shape, analysing the raw IMU readings berfore and after the undetermined time instant. The contributions can be summarized into:\n Robust and adaptive zero velocity detection: Compared with the fixed threshold detector, the proposed Symmetrical-Net can adaptively calculate the probability of being stationary, conserving the manual labour to fine tune the threshold. It also outperforms other competing adaptive detectors, which is attributed to considering contextual information and extended observation scope. Accurate and robust pedestrian tracking: Performance of Symmetrical-Net makes a profound impact on the trajectory reconstruction of the ZUPT-aided INS system. Extensive experiments reveals that the navigation system, assisted with proposed detector, maintains high-accuracy positioning under various individuals in three motion (walking, fast walking, and running).  ","date":1603411200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603411200,"objectID":"a3d811d5861985bffbd8db4de806af2a","permalink":"https://mingkunyang.github.io/publication/symmetrical-net/","publishdate":"2020-10-23T00:00:00Z","relpermalink":"/publication/symmetrical-net/","section":"publication","summary":"*IEEE Sensors Journal* **Mingkun Yang**, Ran Zhu, Zhuoling Xiao* , Bo Yan","tags":["PDR","Intelligent Recognition"],"title":"Symmetrical-Net: Adaptive Zero Velocity Detection for ZUPT-Aided Pedestrian Navigation System","type":"publication"},{"authors":null,"categories":null,"content":"Background Visual Odometry (VO), conducts the ego-motion estimation using on-board camera. In this paper, a learning-based monocular VO is proposed. What\u0026rsquo;s more, with the IMU correction through loose-coupled mechanism, a visual-inertial odometry improves the accuracy of pose estimation further.\nContribution The input of the proposed VO is optical flow extracted by TVNet, and the structure of the learning-based VO is inspired by the DenseNet that is lightweight and effective neural network mainly applied to CV feilds. Utilizing the KF, the inertial positioning is merged into the navigation scheme. The contributions can be summarized into:\n Lightweight and accurate visual odometry: Both parameters amount and average execution time of the proposed VO is lighter than competing approach. In addation, the proposed VO outperforms other VO in translation estimation. Loose-coupled visual-inertial odometry: The well devisd data fusion algorithm considers the accumulated error of INS. Therefore, the penalty for translation estimation by INS is set, which avoids excessive correction. Compared with the proposed visual odometry, the visual-inertial decreases the translation and rotation error by up to 70% and 73%, respectively.  ","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"d8dc9e1fed3da290e1f38d08ceebb9ac","permalink":"https://mingkunyang.github.io/publication/globecom/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/publication/globecom/","section":"publication","summary":"*2019 IEEE Global Communications Conference* (GLOBECOM) Zibin Guo, **Mingkun Yang**, Ninghao Chen, Zhuoling Xiao*, Bo Yan, Shuisheng Lin, Liang Zhou","tags":["Monocular Visual Odometry","Visual-inertial Odometry","Sensor fusing"],"title":"LightVO: Lightweight Inertial-Assisted Monocular Visual Odometry with Dense Neural Networks","type":"publication"},{"authors":null,"categories":null,"content":"Background Human activity recognition (HAR), identifying the actions of individual based on a set of observation, is widely applied to pedestrian indoor tracking, healthcare, Intelligent city, etc. Vision-based recognition algorithms suffer from factors such as lighting condition, clothing color, and image background. Therefore, this paper, leveraging the build-in sensor of the smartphone, proposes a sensor-based HAR model.\nContribution The proposed model utilizes CNN to perform the HAR task. A CNN-7 block is used to identify seven activities. In terms of confusing human activities, a binary-classifier CNN-2, in the ensemble learning way, helps the model to determine the actual human activity through weighted voting. The contributions can be summarized into:\n Abundant and extensive date collection: The data in this paper contains three motion sensors readings collected by 100 participants. There are four different placements of the smartphone and seven typical daily human activities. The data set not only guarantees the effective training on neural network, but makes the recognition task challenging. Accurate and robust recognition: Avoiding handcrafted features engineering, the average accuracy of proposed ensemble model can achieve up to 96.11%. The devised voting mechanism improves the classification accuracy among two confusing activities (walking and going upstairs) further.  ","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"f708b06b2cab65a2875abf88f32482ad","permalink":"https://mingkunyang.github.io/publication/access/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/access/","section":"publication","summary":"*IEEE Access* Ran Zhu, Ying Li, **Mingkun Yang**, Zhuoling Xiao*, Yawen Tan, Liang Zhou, Shuisheng Lin, Hongkai Wen","tags":["HAR","Intelligent Recognition"],"title":"Efficient Human Activity Recognition Solving the Confusing Activities via Deep Ensemble Learning","type":"publication"},{"authors":null,"categories":null,"content":"Background In the GPS-denied environment, inertial navigation system is the justifiable and general solution because it do not require installation of infrastructure beforehand. Nevertheless, the inertial navigation system (INS) calculates the relative displacement of an object that will result in accumulated errors due to the sensorsâ€™ drift. Under the assumption that the velocity is zero when foot is on the ground, ZUPT-aided INS modify the positioning error leveraging this pseudo-measurement.\nContribution This paper improve the ZUPT-aided INS in two aspects: 1) By considering the zero velocity detecion as a hypothesis-testing problem, we propose a new test statistic; 2) To avoid excessive modification of heading angle (yaw), an asymptotic ZUPT is devised.\n Accurate zero velocity detection: This paper takes the difference value of gyroscope as the test statistic, which makes the wave more sharper. The proposed method enable zero velocity detecion to maintain its accuracy in high-dynamic motion. Stable attitude modification: The heading angle, with zero velocity update, is prone to deviation, because the excessive yaw error estimation by KF at the initial update. Therefore, the attenuation factor is applied to the pseudo-measurement, which alleviates the yaw fluctuation after update.  ","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"3f645d7c09d94ed45b5af4c7d793965c","permalink":"https://mingkunyang.github.io/publication/iscas/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/publication/iscas/","section":"publication","summary":"*2019 IEEE International Symposium on Circuits and Systems* (ISCAS) **MingKun Yang**, Jianbo Liang, Zhuoling Xiao*, Bo Yan, Liang Zhou, Shuisheng Lin, Xinchun Liu","tags":["PDR","Intelligent Recognition"],"title":"The Research of Stance-Phase Detection to Improve ZUPT-Aided Pedestrian Navigation System","type":"publication"}]