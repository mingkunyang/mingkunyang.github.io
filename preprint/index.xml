<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>preprint | Mingkun&#39;s homepage</title>
    <link>https://mingkunyang.github.io/preprint/</link>
      <atom:link href="https://mingkunyang.github.io/preprint/index.xml" rel="self" type="application/rss+xml" />
    <description>preprint</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 10 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://mingkunyang.github.io/images/icon_hu786fa58ccadddb09b05070c28e40422f_10189_512x512_fill_lanczos_center_2.png</url>
      <title>preprint</title>
      <link>https://mingkunyang.github.io/preprint/</link>
    </image>
    
    <item>
      <title>MetricNet: A Loop Closure Detection Method for Appearance Variation using Adaptive Weighted Similarity Matrix</title>
      <link>https://mingkunyang.github.io/preprint/metricnet/</link>
      <pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://mingkunyang.github.io/preprint/metricnet/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Loop Closure Detection (LCD), also known as visual place recognition, provides extra information that &lt;strong&gt;robot has visited this area before&lt;/strong&gt;. This constrain assists the positioning system to &lt;strong&gt;reduce the pose estimation error&lt;/strong&gt; by following optimization algorithm.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The proposed MetricNet contains two blocks: 1) feature extraction leveraging &lt;strong&gt;deep learning&lt;/strong&gt;; 2) similarity measurement with &lt;strong&gt;adaptive weight&lt;/strong&gt;. As an &lt;strong&gt;end-to-end&lt;/strong&gt; loop closure detector, MetricNet is trained by minimizing the binary cross-entropy (BCE) consisting of label and calculated similarity score.
The contributions can be summarized into:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Effective feature selection and adaptive similarity matrix&lt;/strong&gt;: The learning-based feature extraction exploits spatial information. Inspired by inverse documentary frequency (IDF), a channel weight mechanism is devised to distil features that are effective for image matching. In terms of similarity calculation, it considers the link between diagonal and off-diagonal elements of similarity matrix.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accurate and robust detection performance&lt;/strong&gt;: Extensive experiments reveal that MetricNet outperforms both learning-based and conventional approaches, especially under the condition with distinct apperrance changes.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual Odometry</title>
      <link>https://mingkunyang.github.io/preprint/deepavo/</link>
      <pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://mingkunyang.github.io/preprint/deepavo/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Visual Odometry (VO), recovers the ego-motion from image sequences by exploiting the consistency between consecutive frames, which has been widely applied to various applications, ranging
from autonomous driving and space exploration to virtual and augmented reality. Although many state-of-the-art learning-based methods have yielded competitive results against classic algorithms, they consider the visual cues in the whole image equally.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The proposed DeepAVO distinguishes and selects extracted features from two aspects: 1) there are four branches extract geometric information from &lt;strong&gt;corresponding quadrant of optical flow&lt;/strong&gt;; 2) each branch in the DeepAVO contains two CBAM blocks enabling the model to concentrate on &lt;strong&gt;pixels in distinct motion&lt;/strong&gt;. The contributions can be summarized into:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Novel visual perception guiding ego-motion estimation&lt;/strong&gt;: The DeepAVO considers the features in four quadrants of optical flow dividually, and fuse the distilling module into each encoder branch. It makes the learning-based model pays more attention to the visual cues that are effective for ego-motion estimation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accurate and robust VO&lt;/strong&gt;: Our medel outperforms many learning-based and traditional monocular VO methods, and even gives &lt;strong&gt;competitive results against the classic stereo VISO2-S algorithm&lt;/strong&gt;. In addation, the DeepAVO produces &lt;strong&gt;promising tracking results on the cross-dataset validation&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Symmetrical-Net: Adaptive Zero Velocity Detection for ZUPT-Aided Pedestrian Navigation System</title>
      <link>https://mingkunyang.github.io/preprint/symmetrical-net/</link>
      <pubDate>Fri, 23 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://mingkunyang.github.io/preprint/symmetrical-net/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Zero velocity detection selects the phase that &lt;strong&gt;foot is anchored to the ground&lt;/strong&gt; from the whole gait of pedestrian. As the vital part of zero velocity update (ZUPT), it &lt;strong&gt;triggers the error-state Kalman Filter (ESKF)&lt;/strong&gt; to modify the position estimated by pedestrian dead reckoning (PDR).&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The characteristics of proposed Symmetrical-Net, as an adaptive zero velocity detector, contain: 1) leveraging &lt;strong&gt;RCNNs&lt;/strong&gt; extracts the features from waveform image of inertial data, and derives connections among consecutive sampling points; 2) Two identical RCNNs branches are &lt;strong&gt;assembled in a symmetrical shape&lt;/strong&gt;, analysing the raw IMU readings berfore and after the undetermined time instant. The contributions can be summarized into:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Robust and adaptive zero velocity detection&lt;/strong&gt;: Compared with the fixed threshold detector, the proposed Symmetrical-Net can &lt;strong&gt;adaptively calculate the probability&lt;/strong&gt; of being stationary, conserving the manual labour to fine tune the threshold. It also outperforms other competing adaptive detectors, which is attributed to &lt;strong&gt;considering contextual information&lt;/strong&gt; and &lt;strong&gt;extended observation scope&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accurate and robust pedestrian tracking&lt;/strong&gt;: Performance of Symmetrical-Net makes a profound &lt;strong&gt;impact on the trajectory reconstruction of the ZUPT-aided INS system&lt;/strong&gt;. Extensive experiments reveals that the navigation system, assisted with proposed detector, maintains high-accuracy positioning &lt;strong&gt;under various individuals in three motion&lt;/strong&gt; (walking, fast walking, and running).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Visual Inertial Map Matching for Indoor Positioning using Architectural Constraints</title>
      <link>https://mingkunyang.github.io/preprint/map/</link>
      <pubDate>Sun, 07 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://mingkunyang.github.io/preprint/map/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For the self-contained navigation systems, the accurate &lt;strong&gt;initial position&lt;/strong&gt;, as the prior knowledge, is crucial for real-world task. Whereas, most existing systems rely on the external signals to achieve initialization, which is contrary to the original intention of positioning whithout pre-installed infrastructure.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The proposed algorithm requires 2D plane map, foot-mounted inertial data, hand-held shot images, realizing a real self-contained navigation system. From the topological points in the maps, the trajectory of pedestrian can be selected via three steps: 1) cursory selection by &lt;strong&gt;ZUPT-aided INS&lt;/strong&gt; generates several candidate paths; 2) &lt;strong&gt;door detection and matching&lt;/strong&gt; excludes some trails further; 3) the final result is decided by a &lt;strong&gt;Siamese Network&lt;/strong&gt; matching the spatial structure.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
