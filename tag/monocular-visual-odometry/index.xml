<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Monocular Visual Odometry | Mingkun&#39;s homepage</title>
    <link>https://mingkunyang.github.io/tag/monocular-visual-odometry/</link>
      <atom:link href="https://mingkunyang.github.io/tag/monocular-visual-odometry/index.xml" rel="self" type="application/rss+xml" />
    <description>Monocular Visual Odometry</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 01 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://mingkunyang.github.io/images/icon_hu786fa58ccadddb09b05070c28e40422f_10189_512x512_fill_lanczos_center_2.png</url>
      <title>Monocular Visual Odometry</title>
      <link>https://mingkunyang.github.io/tag/monocular-visual-odometry/</link>
    </image>
    
    <item>
      <title>LightVO: Lightweight Inertial-Assisted Monocular Visual Odometry with Dense Neural Networks</title>
      <link>https://mingkunyang.github.io/publication/globecom/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://mingkunyang.github.io/publication/globecom/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Visual Odometry (VO), conducts the ego-motion estimation using on-board camera. In this paper, a &lt;strong&gt;learning-based monocular VO&lt;/strong&gt; is proposed. What&amp;rsquo;s more, with the IMU correction through &lt;strong&gt;loose-coupled mechanism&lt;/strong&gt;, a visual-inertial odometry improves the accuracy of pose estimation further.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The input of the proposed VO is optical flow extracted by TVNet, and the structure of the learning-based VO is inspired by the DenseNet that is lightweight and effective neural network mainly applied to CV feilds. Utilizing the &lt;strong&gt;KF&lt;/strong&gt;, the inertial positioning is merged into the navigation scheme. The contributions can be summarized into:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lightweight and accurate visual odometry&lt;/strong&gt;: Both parameters amount and average execution time of the proposed VO is lighter than competing approach. In addation, the proposed VO outperforms other VO in translation estimation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Loose-coupled visual-inertial odometry&lt;/strong&gt;: The well devisd data fusion algorithm considers the accumulated error of INS. Therefore, the &lt;strong&gt;penalty for translation estimation&lt;/strong&gt; by INS is set, which avoids excessive correction. Compared with the proposed visual odometry, the visual-inertial decreases the translation and rotation error by up to &lt;strong&gt;70%&lt;/strong&gt; and &lt;strong&gt;73%&lt;/strong&gt;, respectively.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
