<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Visual Odometry | Mingkun&#39;s homepage</title>
    <link>https://mingkunyang.github.io/tag/visual-odometry/</link>
      <atom:link href="https://mingkunyang.github.io/tag/visual-odometry/index.xml" rel="self" type="application/rss+xml" />
    <description>Visual Odometry</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 05 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://mingkunyang.github.io/images/icon_hu786fa58ccadddb09b05070c28e40422f_10189_512x512_fill_lanczos_center_2.png</url>
      <title>Visual Odometry</title>
      <link>https://mingkunyang.github.io/tag/visual-odometry/</link>
    </image>
    
    <item>
      <title>DeepAVO: Efficient Pose Refining with Feature Distilling for Deep Visual Odometry</title>
      <link>https://mingkunyang.github.io/preprint/deepavo/</link>
      <pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://mingkunyang.github.io/preprint/deepavo/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Visual Odometry (VO), recovers the ego-motion from image sequences by exploiting the consistency between consecutive frames, which has been widely applied to various applications, ranging
from autonomous driving and space exploration to virtual and augmented reality. Although many state-of-the-art learning-based methods have yielded competitive results against classic algorithms, they consider the visual cues in the whole image equally.&lt;/p&gt;
&lt;h2 id=&#34;contribution&#34;&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The proposed DeepAVO distinguishes and selects extracted features from two aspects: 1) there are four branches extract geometric information from &lt;strong&gt;corresponding quadrant of optical flow&lt;/strong&gt;; 2) each branch in the DeepAVO contains two CBAM blocks enabling the model to concentrate on &lt;strong&gt;pixels in distinct motion&lt;/strong&gt;. The contributions can be summarized into:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Novel visual perception guiding ego-motion estimation&lt;/strong&gt;: The DeepAVO consider the features in four quadrants of optical flow dividually, and fuse the distilling module into each encoder branch. It makes the learning-based model pays more attention to the visual cues that are effective for ego-motion estimation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accurate and robust VO&lt;/strong&gt;: Our medel outperforms many learning-based and traditional monocular VO methods, and even gives &lt;strong&gt;competitive results against the classic stereo VISO2-S algorithm&lt;/strong&gt;. In addation, the DeepAVO produces &lt;strong&gt;promising tracking results on the cross-dataset validation&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
